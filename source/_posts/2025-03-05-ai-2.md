---
title: AI 系列:LangChain
tags:
  - AI
categories: AI
date: 2025-03-05 19:45:00
---


### 什么是 LangChain

什么是`LangChain`? `LangChain`一直在快速的演化中，理解`LangChain`有三个层次：

- 开发框架
    - 作为一个框架，`LangChain` 最重要的价值就是提供了一些开发应用所需的基础抽象和`LangChain`表达语言。它将一个一个组件组装在一起，形成一条链，然后调用这条链。LangChain提供了一系列的抽象，比如：模型（`model`）和输出结果解析器（`parser`）。`LangChain`还通过`LangChain`表达式语言(`LangChain Expression Language，简称: LCEL`)来简化编写。

- 社区生态
    - 我们可以把`LangChain`定义的基础抽象理解成一个一个的接口，但是无法完成任何工作，我们还需要这些接口具体的实现。但是这些没有放到`LangChain`核心框架中，需要我们理解`LangChain`的社区生态。社区生态包含大量的实现，比如：模型有`OpenAI`的`GPT`、`Anthropic`的Claude。还有向量数据，`Milvus`、FAISS等。可以在`LangChain`的[集成](https://python.langchain.com/docs/integrations/providers/)页面上查询。除了代码实现，`LangChain`还有一个[提示词社区](https://smith.langchain.com/hub/)，可以找到各类别的提示词。

- 扩展生态
    - 除了`LangChain`构建链外，还有其它的一些，如：`LangServe`把`LangChain`编写的链部署成一个工具。`LangSmith`一个`SaaS`平台，可以帮助开发者调试、追踪、测试、评估和监控大模型应用。`LangGraph`提供了一种构建`Agent`的方式，把`Agent`的状态流转构建成一个图。甚至还有个IDE：`LangGraph Studio`来简化整个构建过程。


### 如何使用LangChain

首先安装`LangChain`的`Python`版本`SDK`：

```bash
pip install langchain-core langchain-openai
```

```python
import os

from langchain_core.message import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "OPENAI API KEY"
os.environ["OPENAI_API_BASE"] = "BASE_URL"

messages = [
    SystemMessage(content="Translate the following from English into Chinese:"),
    HumanMessage(content="Welcome to LLM application development!")
]

model = ChatOpenAI(model="gpt-4o-mini")
result = model.invoke(messages)

print(result)

# ------

content='欢迎来到大语言模型（LLM）应用开发！' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 26, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-1c3cf888-c20d-4863-b1a5-e1e9ecfbe33d-0' usage_metadata={'input_tokens': 26, 'output_tokens': 13, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}

```

上面我们使用`OpenAI`模型，使用了`langchain-openai`这个`SDK`，`LangChain`社区统一把它们称为**供应商(Provider)**，每条消息都包含角色和内容两个部分，`SystemMessage`表示消息角色是系统，`HumanMessage`表示消息的角色是人。然后我们通过`model.invoke`把消息传给模型。

我们也可以使用流式响应。只需要把`result = model.invoke(messages)` 改成 `stream = model.stream(message)`，处理方式改成：`for i in stream: print(i.content)`

**PromptTemplate**
想用好大模型，提示词是非常关键的，许多大模型应该本质上就是开发者预置好提示词，把它和用户的提示词结合在一起发给大模型，以便达到更好的效果。`LangChain`把这种预置提示词的方法提炼了出来，引入了提示词模板(`PromptTemplate`)的概念。

```python
import os

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "OPENAI API KEY"
os.environ["OPENAI_API_BASE"] = "BASE_URL"

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "Translate the following from English into Chinese:"),
        ("user", "{text}")
    ]
)

model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt_template | model

stream = chain.stream({"text": "Welcome to LLM application development!"})

for i in stream:
    print(i.content, end="")

```
上面代码中把`PromptTemplate`和`ChatModel`两个组件通过`LCEL`把它们组成一条链。

**OutputParser**
`PromptTemplate`处理的输入，与之对应的是`OutputParser`，它是负责处理输出的。对上面例子，把输出结果拿出来就对应着`StrOutputParser`。

```python
import os

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "OPENAI API KEY"
os.environ["OPENAI_API_BASE"] = "BASE_URL"

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "Translate the following from English into Chinese:"),
        ("user", "{text}")
    ]
)

model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt_template | model | parser
stream = chain.stream({"text": "Welcome to LLM application development!"})

for i in stream:
    print(i, end="")

```

我们也可使用其它的输出，比如：`JsonOutputParser`。


### 使用LangChain创建一个简单的聊天机器人


```python
import os

import tiktoken
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage, trim_messages
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "OPENAI API KEY"
os.environ["OPENAI_API_BASE"] = "BASE_URL"


chat_model = ChatOpenAI(model="gpt-4o-mini")
store = {}

def get_session_history(session_id):
    """
    获取会话的历史记录
    """
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()

    return store[session_id]


def str_token_counter(text):
    enc = tiktoken.get_encoding("o200k_base")
    return len(enc.encode(text))


def tiktoken_counter(messages):
    """
    统计token数量
    """
    num_tokens = 3
    token_pre_message = 3
    tokens_pre_name = 1

    for msg in messages:
        if isinstance(msg, HumanMessage):
            role = "user"
        elif isinstance(msg, AIMessage):
            role = "assistant"
        elif isinstance(msg, ToolMessage):
            role = "tool"
        elif isinstance(msg, SystemMessage):
            role = "system"
        else:
            raise ValueError(f"Unsupported message type {msg.__class__}")
        num_tokens += (
            token_pre_message
            + str_token_counter(role)
            + str_token_counter(msg.content)
        )
        if msg.name:
            num_tokens += tokens_pre_name + str_token_counter(msg.name)
    return num_tokens


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "你现在扮演老子角色，尽量按照老子的风格回复，请使用白话文回答"),
        MessagesPlaceholder(variable_name="messages")
    ]
)

# 避免历史过长
trimmer = trim_messages(
    max_tokens=4096,
    strategy="last",
    token_counter=tiktoken_counter,
    include_system=True
)

with_message_history = RunnableWithMessageHistory(
    trimmer | prompt | chat_model,
    get_session_history
)

config = {"configurable": {"session_id": "cc"}}

while True:
    user_input = input("You:> ")
    if user_input.lower() == "exit":
        break

    stream = with_message_history.stream(
        {"messages": [HumanMessage(content=user_input)]},
        config=config
    )

    for i in stream:
        print(i.content, end="", flush=True)

    print()

```

```bash
You:>什么是人生？
人生者，天地之道也。似水流云，无常而常，顺其自然，方得其乐。知足者常足，心静者常安。非求而得，非执而持，随遇而安，体悟无为，方显真谛。人生如梦，梦如人生，处之泰然，心中自有天地。
You:>如何过好一生？
过好一生，宜顺应自然，心存柔和。知己之心，明其所需，勿因欲望而扰。修身齐家，待人以诚，心静而和，乃能得安宁。

常与自然为友，观花听风，简朴而知足，心中无挂，身心自乐。勿争名利，勿问长生，且行且珍惜，日月轮回，时光荏苒，保留一颗初心，乐在当下，便是过好一生。
You:>exit
```
