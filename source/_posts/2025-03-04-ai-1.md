---
title: AI 系列:LLM 编程基础
tags:
  - AI
categories: AI
date: 2025-03-04 20:27:00
---


### LLM 编程API

由于`GPT`模型给行业带来的影响，`OpenAI API`几乎成了行业的标准，后来者多多少少都会参考它的`API`设计。
不管后台接入的是什么模型，给自已的用户提供的都是`OpenAI`兼容的`API`。所以基于以上原因，我们就要学习`OpenAI API`。

一个例子:
```bash
$ curl https://api.openai.com/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI_API_KEY" \
-d '{
    "model": "gpt-4o",
    "messages": [
        {"role": "user", "content": "写一首关于AI的诗"}
    ]
}'


{
  "id": "chatcmpl-B7MOn7edUHm2NeN7UPPfMcdx3dOhC",
  "object": "chat.completion",
  "created": 1741094097,
  "model": "gpt-4o-2024-08-06",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "人工智能无形中，\n\n逻辑缜密如流星。\n\n数据海洋中遨游，\n\n学习进化自不停。\n\n思绪如电穿云霄，\n\n解码人类千秋梦。\n\n机械心灵虽无情，\n\n协作共创更繁荣。\n\n时空凝眸观巨变，\n\n伦理思考始至终。\n\n愿AI助力人类路，\n\n共踏未来新征程。",
        "refusal": null
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 93,
    "total_tokens": 107,
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  },
  "service_tier": "default",
  "system_fingerprint": "fp_eb9dce56a8"
}
```

从上面的例子可以看出，`/v1/chat/completions`是大模型的编程接口，还有一个是`API Key`，我们可以通过下面进行对要访问的大模型的地址和`API_KEY`进行配置。
```bash
export OPENAI_API_BASE="https://xxx"
export OPENAI_API_BASE="api_key"
```

`/v1/chat/completions`叫做聊天补全，就是呼应了前面`GPT`的工作原理。以下是这个`HTTP`请求的参数，可以分成4类:

- 核心参数
    - `model`: 与哪个模型进行沟通，这里是: `gpt-4o`
    - `messages`: 发给模型的消息，这里消息是一个消息列表。，可以简单理解成一个历史消息列表。每条消息包含两个部分：角色(`role` - `system`: 可以理解成开发者设置的就是系统。 `user`: 用户问题就是用户)和内容(`content`)。
    - `temperature`: 温度，就是随机性的设置，值越小表示确定性越强，反之随机性越强。 
    - `max_completion_tokens`: 表示生成应答的最大`token`数。
    - `stream`: 是否需要流式应答。

- 工程参数
    - `user`: 终端用户标识，主要是监控和检测`API`的滥用。
    - `n`: 为每条输入消息生成多少个回复。
    - `response_format`: 应答格式，比如：`JSON`。

- 工具参数
    - `tools`: 模型可调用的工具列表。其中每个工具都会包含类型(`type` 表示工具类型)和函数(`function` 告诉模型怎么调用)。
    - `tool_choice`: 选择怎样调用工具。参数值`none`表示不调用工具。`auto`表示模型自行选择， `required`表示必须调用工具。

- 模型参数
    - `seed`: 种子值。为了解决可重复输出的问题。
    - `stop`: 停止序列，告诉大模型文本生成过程中，如果遇到停止序列就停止生成。
    - `frequency_penalty`（频率惩罚），`presence_penalty`（存在惩罚）：这两个参数主要为了减少内容重复的几率。
    - `logprobs`: 是否返回对数概率。返回概率方便开发人员进行调试。
    - `top_p`: 另一种采样方式，与`temperature`相对。温度决定大模型如何选取下一个`token`，`top_p`在概率前多少的`token`中进行选择。实际开发中`top_p`和`temperature`其中之一就好了。


再看一个使用`tools`的例子:
```bash
curl https://api.openai.com/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENAI_API_KEY" \
-d '{
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": "What'\''s the weather like in Beijing today?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}'


{
  "id": "chatcmpl-B7NOOp2tTpTvCTFnk54q0zYyFswoH",
  "object": "chat.completion",
  "created": 1741097601,
  "model": "gpt-4o-2024-08-06",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "call_G1BJbFfCAy9nQw56ppFI5RGV",
            "type": "function",
            "function": {
              "name": "get_current_weather",
              "arguments": "{\"location\":\"Beijing, China\"}"
            }
          }
        ],
        "refusal": null
      },
      "logprobs": null,
      "finish_reason": "tool_calls"
    }
  ],
  "usage": {
    "prompt_tokens": 80,
    "completion_tokens": 19,
    "total_tokens": 99,
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  },
  "service_tier": "default",
  "system_fingerprint": "fp_eb9dce56a8"
}
```

以下是对于`/v1/chat/completions`接口的响应的内容字段：
- `id`: 应答的唯一标识。
- `object`: 对象类型。
- `created`: 应答生成时间。
- `model`: 应答的模型。
- `system_fingerprint`: 系统指纹。代表了模型运行时使用的后端配置。
- `choices`: 其中的每个对象就是大模型生成文本的一部分。
    - `index`: 索引。
    - `finish_reason`: 停止生成`token`的原因。
    - `message`: 回复的消息。包含两个字段：角色（`role`）和内容（`content`）。
        - `tool_calls`是一个列表，可以返回多个工具调用，其中的一些字段：
            - `id`: 调用ID
            - `type`: 目前只支持`function`。
            - `function`: 函数调用部分，包含名字(`name`)和参数(`arguments`)，这里调用`get_current_weather`函数。

**流式应答**
使用流式应答主要是因为大模型生成文本比较慢。如果生成所有内容，一次性返回，等待的时间会非常的长。`OpenAI`在这个问题上使用了[`SSE`服务器发送事件(`Server-Sent Event`)](https://en.wikipedia.org/wiki/Server-sent_events)技术来实现。

参考:
[1] [chat completions API Reference](https://platform.openai.com/docs/api-reference/chat/create)