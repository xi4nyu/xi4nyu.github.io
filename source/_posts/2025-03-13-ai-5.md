---
title: AI 系列:模型微调
tags:
  - AI
categories: AI
date: 2025-03-15 20:25:00
---


### 什么是模型微调，为什么要微调?

1. 让大模型了解我们的业务有两种方法：（1）RAG，(2) 模型微调。
2. 如果可以的话，我们都想有一个属于自己的模型，但是训练一个模型需要花太多钱。所以我们要基于训练好的开源模型进行一部分重新调整。

### 如何进行微调

**模型微调通常分为以下几个步骤**

1. 准备训练数据
2. 训练模型
3. 评估结果
4. 使用模型

**1. 格式说明**
在对模型进行微调时还需要确定下面两点：
1. 要准备数据，通常准备数据需要懂业务人员配合。还需要了解数据的格式。
    - `OpenAI`的模型微调格式
    ```json
    {"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "Paris, as if everyone doesn't know that already."}]}
    {"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?"}]}
    {"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters."}]}
    ```
    - 我们使用[LLaMa Factory](https://github.com/hiyouga/LLaMA-Factory)，它支持的格式：`alpaca`和`sharegpt`，[格式说明](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md)。
    ```json
    // Alpaca
    [
        {
            "instruction": "human instruction (required)",
            "input": "human input (optional)",
            "output": "model response (required)",
            "system": "system prompt (optional)",
            "history": [
            ["human instruction in the first round (optional)", "model response in the first round (optional)"],
            ["human instruction in the second round (optional)", "model response in the second round (optional)"]
            ]
        }
    ]
    // Sharegpt
    [
        {
            "conversations": [
            {
                "from": "human",
                "value": "human instruction"
            },
            {
                "from": "function_call",
                "value": "tool arguments"
            },
            {
                "from": "observation",
                "value": "tool result"
            },
            {
                "from": "gpt",
                "value": "model response"
            }
            ],
            "system": "system prompt (optional)",
            "tools": "tool description (optional)"
        }
    ]
    ```

2. 闭源模型要进行微调，就需要通过接口把数据提交给闭源模型（OpenAI：[模型微调服务](https://platform.openai.com/docs/guides/fine-tuning/)）。所以很多的团队实际的做法是基于开源的模型进行微调。

**使用LLaMa Factory进行微调**

1. 安装
```bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
conda create -n llama-factory python=3.12
conda activate llama-factory
pip install -e ".[torch,metrics]"

# 测试是否安装成功
llamafactory-cli version
```
2. 准备训练数据(`alpaca`和`sharegpt`)
```json
[
    {
        "instruction": "",
        "input": "",
        "output": ""
    }
]
```
3. 训练
```bash
# 启动图形界面
llamafactory-cli webui
```
要先测试微调功能，我们可以先在关注几个参数：模型名称，数据集，输出目录；我们还需要下载一个开源的模型，我们这里以`Qwen2.5-0.5B`为例，可以通过`HuggingFace`下载，也可以通过国内`ModelScope`下载。下载好后，指定模型名称和模型目录，然后点击开始，就开始训练了。
```bash
pip install -U "huggingface_hub[cli]"
huggingface-cli login
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
```
![LLaMa Factory WebUI](/images/ai/llama-factory-webui.png)

训练完成后，我们可以通过`LLaMa Factory`加载模型进行测试。

![LLaMa Factory Chat](/images/ai/llama-factory-chat.jpg)

