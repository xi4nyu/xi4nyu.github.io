---
title: AI 系列:RAG
tags:
  - AI
categories: AI
date: 2025-03-06 19:22:00
---


### 什么是 RAG

`RAG` 是 `Retrieval-Augmented Generation`，也就是检索增强生成。如果我们想为自己的业务开发一个聊天机器人，让机器人知道我们的业务，常见的解决方案有两种：
- 模型微调：使用业务信息对已经训练好的模型进行微调。
- RAG：在上下文中带有业务信息，让大模型据此进行整合。

### 向量数据库
大模型主要是对向量进行处理，所以通过将用户的问题转换成向量，然后计算向量之间距离，找到与问题向量最接近的文档向量，从而实现“语义”的匹配。后再到大模型去完成生成。
`OpenAI`提供了一个专门负责将文档转换成向量的`API`：[Embedding](https://platform.openai.com/docs/api-reference/embeddings) 

### 一个示例
LangChain [Build a Retrieval Augmented Generation (RAG) App](https://python.langchain.com/v0.2/docs/tutorials/rag/)

安装`SDK`: 
```bash
pip install langchain_community beautifulsoup4 langchain-chroma
```

```python
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader

py_doc_url = "https://docs.python.org/3/tutorial/index.html"
loader = WebBaseLoader(py_doc_url)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = Chroma(
    collection_name="pydocs",
    embedding_function=OpenAIEmbeddings(),
    persist_directory="vectordb"
)
vectorstore.add_documents(splits)

documents = vectorstore.similarity_search("How to use data structures in Python?")
print(documents)

```

在{% post_link 2025-03-05-ai-2 LangChain %}中一个聊天机器，我们添加RAG功能，让它可以回答我们的业务问题。

```python
import os
from operator import itemgetter

import tiktoken
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage, trim_messages
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.vectorstores import Chroma


vectorstore = Chroma(
    collection_name="pydocs",
    embedding_function=OpenAIEmbeddings(),
    persist_directory="vectordb"
)
retriever = vectorstore.as_retriever(search_type="similarity")
chat_model = ChatOpenAI(model="gpt-4o-mini")
store = {}

def get_session_history(session_id):
    """
    获取会话的历史记录
    """
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()

    return store[session_id]


def str_token_counter(text):
    enc = tiktoken.get_encoding("o200k_base")
    return len(enc.encode(text))


def tiktoken_counter(messages):
    """
    统计token数量
    """
    num_tokens = 3
    token_pre_message = 3
    tokens_pre_name = 1

    for msg in messages:
        if isinstance(msg, HumanMessage):
            role = "user"
        elif isinstance(msg, AIMessage):
            role = "assistant"
        elif isinstance(msg, ToolMessage):
            role = "tool"
        elif isinstance(msg, SystemMessage):
            role = "system"
        else:
            raise ValueError(f"Unsupported message type {msg.__class__}")
        num_tokens += (
            token_pre_message
            + str_token_counter(role)
            + str_token_counter(msg.content)
        )
        if msg.name:
            num_tokens += tokens_pre_name + str_token_counter(msg.name)
    return num_tokens


trimmer = trim_messages(
    max_tokens=4096,
    strategy="last",
    token_counter=tiktoken_counter,
    include_system=True
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", """You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Responed in chinese. Context: {context}"""),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ]
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

context = itemgetter("question") | retriever | format_docs
first_step = RunnablePassthrough.assign(context=context)
chain = first_step | prompt | trimmer | chat_model
config = {"configurable": {"session_id": "cc"}}

with_message_history = RunnableWithMessageHistory(
    chain,
    get_session_history=get_session_history,
    input_messages_key="question",
    history_messages_key="history"
)

while True:
    user_input = input("You:> ")
    if user_input.lower() == "exit":
        break

    if user_input.strip() == "":
        continue

    stream = with_message_history.stream(
        {"question": user_input},
        config=config
    )

    for i in stream:
        print(i.content, end="", flush=True)

    print()


```